# -*- coding: utf-8 -*-
"""Copy of Eda& clustering_masterdata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j53tIdhi5CTJGoIuMUHIZuh9M5AoJE2s
"""

import numpy as np
import pandas as pd

df=pd.read_csv('/content/master.csv')

df

df.columns

col=['ACS_AVG_HH_SIZE',
'ACS_GINI_INDEX',
'ACS_MEDIAN_AGE',
'ACS_MEDIAN_AGE_FEMALE',
'ACS_MEDIAN_AGE_MALE',
'ACS_MEDIAN_HH_INC',
'ACS_MEDIAN_HH_INC_AIAN',
'ACS_MEDIAN_HH_INC_ASIAN',
'ACS_MEDIAN_HH_INC_BLACK',
'ACS_MEDIAN_HH_INC_HISP',
'ACS_MEDIAN_HH_INC_MULTI',
'ACS_MEDIAN_HH_INC_NHPI',
'ACS_MEDIAN_HH_INC_OTHER',
'ACS_MEDIAN_HH_INC_WHITE',
'ACS_MEDIAN_INC_F',
'ACS_MEDIAN_INC_M',
'ACS_MEDIAN_NONVET_INC',
'ACS_MEDIAN_VET_INC',
'ACS_PCT_AGE_0_17',
'ACS_PCT_AGE_0_4',
'ACS_PCT_AGE_10_14',
'ACS_PCT_AGE_15_17',
'ACS_PCT_AGE_18_29',
'ACS_PCT_AGE_18_44',
'ACS_PCT_AGE_30_44',
'ACS_PCT_AGE_45_64',
'ACS_PCT_AGE_50_64',
'ACS_PCT_AGE_5_9',
'ACS_PCT_AGE_ABOVE65',
'ACS_PCT_AGE_ABOVE80',
'ACS_PCT_AIAN',
'ACS_PCT_AIAN_COMB',
'ACS_PCT_AIAN_FEMALE',
'ACS_PCT_AIAN_MALE',
'ACS_PCT_AIAN_NONHISP',
'ACS_PCT_API_LANG',
'ACS_PCT_ASIAN',
'ACS_PCT_ASIAN_COMB',
'ACS_PCT_ASIAN_FEMALE',
'ACS_PCT_ASIAN_MALE',
'ACS_PCT_ASIAN_NONHISP',
'ACS_PCT_BACHELOR_DGR',
'ACS_PCT_BLACK',
'ACS_PCT_BLACK_COMB',
'ACS_PCT_BLACK_FEMALE',
'ACS_PCT_BLACK_MALE',
'ACS_PCT_BLACK_NONHISP',
'ACS_PCT_CHILDREN_GRANDPARENT',
'ACS_PCT_CHILD_1FAM',
'ACS_PCT_CHILD_DISAB',
'ACS_PCT_COLLEGE_ASSOCIATE_DGR',
'ACS_PCT_COMMT_15MIN',
'ACS_PCT_COMMT_29MIN',
'ACS_PCT_COMMT_59MIN',
'ACS_PCT_COMMT_60MINUP',
'ACS_PCT_DISABLE',
'ACS_PCT_ENGL_NOT_ALL',
'ACS_PCT_ENGL_NOT_WELL',
'ACS_PCT_FOREIGN_BORN',
'ACS_PCT_GRADUATE_DGR',
'ACS_PCT_HEALTH_INC_138_199',
'ACS_PCT_HEALTH_INC_200_399',
'ACS_PCT_HEALTH_INC_ABOVE400',
'ACS_PCT_HEALTH_INC_BELOW137',
'ACS_PCT_HH_1PERS',
'ACS_PCT_HH_ABOVE65',
'ACS_PCT_HH_ALONE_ABOVE65',
'ACS_PCT_HH_BROADBAND',
'ACS_PCT_HH_BROADBAND_ANY',
'ACS_PCT_HH_BROADBAND_ONLY',
'ACS_PCT_HH_CELLULAR',
'ACS_PCT_HH_CELLULAR_ONLY',
'ACS_PCT_HH_DIAL_INTERNET_ONLY',
'ACS_PCT_HH_FOOD_STMP',
'ACS_PCT_HH_FOOD_STMP_BLW_POV',
'ACS_PCT_HH_INC_10000',
'ACS_PCT_HH_INC_100000',
'ACS_PCT_HH_INC_14999',
'ACS_PCT_HH_INC_24999',
'ACS_PCT_HH_INC_49999',
'ACS_PCT_HH_INC_99999',
'ACS_PCT_HH_INTERNET',
'ACS_PCT_HH_INTERNET_NO_SUBS',
'ACS_PCT_HH_LIMIT_ENGLISH',
'ACS_PCT_HH_NO_COMP_DEV',
'ACS_PCT_HH_NO_FD_STMP_BLW_POV',
'ACS_PCT_HH_NO_INTERNET',
'ACS_PCT_HH_OTHER_COMP',
'ACS_PCT_HH_OTHER_COMP_ONLY',
'ACS_PCT_HH_PC',
'ACS_PCT_HH_PC_ONLY',
'ACS_PCT_HH_PUB_ASSIST',
'ACS_PCT_HH_SAT_INTERNET',
'ACS_PCT_HH_SMARTPHONE',
'ACS_PCT_HH_SMARTPHONE_ONLY',
'ACS_PCT_HH_TABLET',
'ACS_PCT_HH_TABLET_ONLY',
'ACS_PCT_HS_GRADUATE',
'ACS_PCT_HU_MOBILE_HOME',
'ACS_PCT_HU_NO_VEH',
'ACS_PCT_LT_HS',
'ACS_PCT_MEDICAID_ANY',
'ACS_PCT_MEDICAID_ANY_BELOW64',
'ACS_PCT_MEDICARE_ONLY',
'ACS_PCT_MULT_RACE',
'ACS_PCT_NONVET_DISABLE_18_64',
'ACS_PCT_OTHER_INS',
'ACS_PCT_OWNER_HU_COST_30PCT',
'ACS_PCT_OWNER_HU_COST_50PCT',
'ACS_PCT_POSTHS_ED',
'ACS_PCT_PRIVATE_ANY',
'ACS_PCT_PRIVATE_ANY_BELOW64',
'ACS_PCT_PRIVATE_EMPL',
'ACS_PCT_PRIVATE_EMPL_BELOW64',
'ACS_PCT_PRIVATE_MDCR',
'ACS_PCT_PRIVATE_MDCR_35_64',
'ACS_PCT_PRIVATE_OTHER',
'ACS_PCT_PRIVATE_OTHER_BELOW64',
'ACS_PCT_PRIVATE_SELF',
'ACS_PCT_PRIVATE_SELF_BELOW64',
'ACS_PCT_PUBLIC_OTHER',
'ACS_PCT_PUBLIC_ONLY',
'ACS_PCT_PUBLIC_OTHER_BELOW64',
'ACS_PCT_PUBL_TRANSIT',
'ACS_PCT_PUB_COMMT_15MIN',
'ACS_PCT_PUB_COMMT_29MIN',
'ACS_PCT_PUB_COMMT_59MIN',
'ACS_PCT_PUB_COMMT_60MINUP',
'ACS_PCT_PVT_EMPL_DRCT',
'ACS_PCT_PVT_EMPL_DRCT_BELOW64',
'ACS_PCT_RENTER_HU_ABOVE65',
'ACS_PCT_RENTER_HU_COST_30PCT',
'ACS_PCT_RENTER_HU_COST_50PCT',
'ACS_PCT_SELF_MDCR_ABOVE35',
'ACS_PCT_TAXICAB_2WORK',
'ACS_PCT_TRICARE_VA',
'ACS_PCT_TRICARE_VA_BELOW64',
'ACS_PCT_UNEMPLOY',
'ACS_PCT_UNINSURED',
'ACS_PCT_UNINSURED_BELOW64',
'ACS_PCT_VET',
'ACS_PCT_VET_BACHELOR',
'ACS_PCT_VET_COLLEGE',
'ACS_PCT_VET_DISABLE_18_64',
'ACS_PCT_VET_HS',
'ACS_PCT_VET_LABOR_FORCE_18_64',
'ACS_PCT_VET_POV_18_64',
'ACS_PCT_VET_UNEMPL_18_64',
'ACS_PCT_WALK_2WORK',
'ACS_PCT_WHITE',
'ACS_PCT_WORK_NO_CAR',
'ACS_PER_CAPITA_INC',
'ACS_TOT_CIVIL_VET_POP',
'ACS_TOT_CIVIL_VET_POP_ABOVE25',
'ACS_TOT_GRANDCHILDREN_GP',
'ACS_TOT_HH',
'ACS_TOT_HU',
'ACS_TOT_POP_16_19',
'ACS_TOT_POP_ABOVE15',
'ACS_TOT_POP_ABOVE16',
'ACS_TOT_POP_ABOVE25',
'ACS_TOT_POP_ABOVE5',
'ACS_TOT_POP_US_ABOVE1',
'ACS_TOT_POP_WT',
'ACS_TOT_WORKER_NWFH'
]

df[col].isnull().sum()

#replacing nan values
df[col] = df[col].fillna(df.median())

from sklearn.preprocessing import MinMaxScaler



# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Apply MinMaxScaler to the numerical columns
df[col] = scaler.fit_transform(df[col])

# Now df contains the scaled values

df.iloc[:, 9:].head()

df=df[:5000]

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Assuming 'X' is your data

# Calculate WCSS for different values of k
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(df[col])
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method graph
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import euclidean_distances
# Assuming df is defined somewhere else in your code

# Handle NaN values in feature matrix
features = df.iloc[:, 9:].values
nan_columns = df.columns[9:][np.isnan(features).any(axis=0)]

nan_columns_count = df.isnull().sum()

# Replace NaN values with mean of respective columns
features = np.nan_to_num(features, nan=np.nanmean(features, axis=0))

# Scale feature data
features_scaled = StandardScaler().fit_transform(features)

# Preprocess geolocation column to extract latitude and longitude
df['Latitude'] = df['Geolocation'].str.extract(r'\((-?\d+\.\d+)')
df['Longitude'] = df['Geolocation'].str.extract(r'(-?\d+\.\d+)\)')

# Convert extracted latitude and longitude to float
df['Latitude'] = df['Latitude'].astype(float)
df['Longitude'] = df['Longitude'].astype(float)

# Scale latitude and longitude data
scaler_geo = StandardScaler()
geo_scaled = scaler_geo.fit_transform(df[['Latitude', 'Longitude']])

# Compute euclidean distances between latitude-longitude points
distances = euclidean_distances(geo_scaled)

# Perform K-means clustering
k = 4 # Number of clusters
kmeans = KMeans(n_clusters=k)
cluster_labels = kmeans.fit_predict((features_scaled))

# Assign cluster labels back to the dataframe
df['Cluster'] = cluster_labels

# Plot clusters
plt.figure(figsize=(10, 8))
for cluster in range(k):
    cluster_df = df[df['Cluster'] == cluster]
    plt.scatter(cluster_df['Longitude'], cluster_df['Latitude'], label=f'Cluster {cluster}')

plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('K-Means Clustering based on Features and Distance')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import euclidean_distances

# Assuming df is defined somewhere else in your code

# Handle NaN values in feature matrix
features = df.iloc[:, 9:].values
nan_columns = df.columns[9:][np.isnan(features).any(axis=0)]

nan_columns_count = df.isnull().sum()

# Replace NaN values with mean of respective columns
features = np.nan_to_num(features, nan=np.nanmean(features, axis=0))

# Scale feature data
features_scaled = StandardScaler().fit_transform(features)

# Compute euclidean distances between latitude-longitude points
scaler_geo = StandardScaler()
geo_scaled = scaler_geo.fit_transform(df[['Latitude', 'Longitude']])
distances = euclidean_distances(geo_scaled)

# Calculate inertia for different values of k
inertia = []
k_values = range(1, 11)  # Test for a range of k values
for k in k_values:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit((features_scaled))
    inertia.append(kmeans.inertia_)

# Plot elbow plot
plt.plot(k_values, inertia, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Plot for K-Means Clustering')
plt.xticks(k_values)
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
import matplotlib.pyplot as plt

# Assuming df is defined somewhere else in your code

# Handle NaN values in feature matrix
features = df.iloc[:, 9:].values
nan_columns = df.columns[9:][np.isnan(features).any(axis=0)]
nan_columns_count = df.isnull().sum()

# Replace NaN values with mean of respective columns
features = np.nan_to_num(features, nan=np.nanmean(features, axis=0))

# Scale feature data
features_scaled = StandardScaler().fit_transform(features)

# Preprocess geolocation column to extract latitude and longitude
df['Latitude'] = df['Geolocation'].str.extract(r'\((-?\d+\.\d+)')
df['Longitude'] = df['Geolocation'].str.extract(r'(-?\d+\.\d+)\)')

# Convert extracted latitude and longitude to float
df['Latitude'] = df['Latitude'].astype(float)
df['Longitude'] = df['Longitude'].astype(float)

# Scale latitude and longitude data
scaler_geo = StandardScaler()
geo_scaled = scaler_geo.fit_transform(df[['Latitude', 'Longitude']])

# Compute euclidean distances between latitude-longitude points
distances = euclidean_distances(geo_scaled)

# Calculate inertia for different values of k
inertia = []
k_values = range(1, 11)  # Test for a range of k values
for k in k_values:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(np.hstack((features_scaled, distances)))  # Concatenate features and distances
    inertia.append(kmeans.inertia_)

# Plot elbow plot
plt.plot(k_values, inertia, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Plot for K-Means Clustering based on Features and Distance')
plt.xticks(k_values)
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances

# Assuming df is defined somewhere else in your code

# Handle NaN values in feature matrix
features = df.iloc[:, 9:].values
nan_columns = df.columns[9:][np.isnan(features).any(axis=0)]
nan_columns_count = df.isnull().sum()

# Replace NaN values with mean of respective columns
features = np.nan_to_num(features, nan=np.nanmean(features, axis=0))

# Scale feature data
features_scaled = StandardScaler().fit_transform(features)

# Preprocess geolocation column to extract latitude and longitude
df['Latitude'] = df['Geolocation'].str.extract(r'\((-?\d+\.\d+)')
df['Longitude'] = df['Geolocation'].str.extract(r'(-?\d+\.\d+)\)')

# Convert extracted latitude and longitude to float
df['Latitude'] = df['Latitude'].astype(float)
df['Longitude'] = df['Longitude'].astype(float)

# Scale latitude and longitude data
scaler_geo = StandardScaler()
geo_scaled = scaler_geo.fit_transform(df[['Latitude', 'Longitude']])

# Compute euclidean distances between latitude-longitude points
distances = euclidean_distances(geo_scaled)

# Perform K-means clustering
k = 4  # Number of clusters
kmeans = KMeans(n_clusters=k)
cluster_labels = kmeans.fit_predict(np.hstack((features_scaled, distances)))

# Assign cluster labels back to the dataframe
df['Cluster_Labels'] = cluster_labels

# Create separate DataFrames for each cluster
df0 = df[df['Cluster_Labels'] == 0]
df1 = df[df['Cluster_Labels'] == 1]
df2 = df[df['Cluster_Labels'] == 2]
df3 = df[df['Cluster_Labels'] == 3]
# Now you have separate DataFrames for each cluster

import pandas as pd  ACS_PCT_COLLEGE_ASSOCIATE_DGR
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df1, df2, df3, df4 are your four dataframes

# Concatenate the dataframes into a single dataframe
combined_df = pd.concat([df0['ACS_PER_CAPITA_INC'], df1['ACS_PER_CAPITA_INC'], df2['ACS_PER_CAPITA_INC']],
                        axis=1, keys=['DF0', 'DF1', 'DF2'])

# Melt the dataframe to long format
melted_df = combined_df.melt(var_name='Source', value_name='ACS_PER_CAPITA_INC')

# Create violin plot
plt.figure(figsize=(10, 6))
sns.violinplot(x='Source', y='ACS_PER_CAPITA_INC', data=melted_df)
plt.title('Comparative Violin Plot for Obesity across Data Frames')
plt.xlabel('Data Frame')
plt.ylabel('ACS_PER_CAPITA_INC')
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# List of features to iterate over
  # Add all features here

# Initialize an empty list to store the plots
plt.figure(figsize=(12, 8))

# Iterate over each feature
for feature in col:
    # Concatenate the dataframes into a single dataframe
    combined_df = pd.concat([df0[feature], df1[feature], df2[feature],df3[feature]],
                            axis=1, keys=['DF0', 'DF1', 'DF2','DF3'])

    # Melt the dataframe to long format
    melted_df = combined_df.melt(var_name='Source', value_name=feature)

    # Create violin plot
    plt.figure(figsize=(10, 6))
    sns.violinplot(x='Source', y=feature, data=melted_df)
    plt.title(f'Comparative Violin Plot for {feature} across Data Frames')
    plt.xlabel('Data Frame')
    plt.ylabel(feature)
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df1, df2, df3, df4 are your four dataframes

# Concatenate the dataframes into a single dataframe
combined_df = pd.concat([df0['ACS_PCT_BACHELOR_DGR'], df1['ACS_PCT_BACHELOR_DGR'], df2['ACS_PCT_BACHELOR_DGR']],
                        axis=1, keys=['DF0', 'DF1', 'DF2'])

# Melt the dataframe to long format
melted_df = combined_df.melt(var_name='Source', value_name='ACS_PCT_BACHELOR_DGR')

# Create violin plot
plt.figure(figsize=(10, 6))
sns.violinplot(x='Source', y='ACS_PCT_BACHELOR_DGR', data=melted_df)
plt.title('Comparative Violin Plot for Obesity across Data Frames')
plt.xlabel('Data Frame')
plt.ylabel('ACS_PCT_BACHELOR_DGR')
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df1, df2, df3 are your three dataframes
dfs = [df0, df1, df2,df3]
df_names = ['df1', 'df2', 'df3','df4']

# List of features you want to plot


# Iterate over each feature
for feature in col:
    plt.figure(figsize=(8, 6))
    for df, name in zip(dfs, df_names):
        sns.kdeplot(df[feature], label=name)
    plt.title(f'Kernel Density Plot for {feature}')
    plt.xlabel(feature)
    plt.ylabel('Density')
    plt.legend()
    plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import networkx as nx

# Assuming df is defined somewhere else in your code

# Handle NaN values in feature matrix
features = df.iloc[:, 9:].values
nan_columns = df.columns[9:][np.isnan(features).any(axis=0)]

nan_columns_count = df.isnull().sum()

# Replace NaN values with mean of respective columns
features = np.nan_to_num(features, nan=np.nanmean(features, axis=0))

# Scale feature data
features_scaled = StandardScaler().fit_transform(features)

# Compute cosine similarities of the features
cosine_sim = cosine_similarity(features_scaled)

# Preprocess geolocation column to extract latitude and longitude
df['Latitude'] = df['Geolocation'].str.extract(r'\((-?\d+\.\d+)')
df['Longitude'] = df['Geolocation'].str.extract(r'(-?\d+\.\d+)\)')

# Convert extracted latitude and longitude to float
df['Latitude'] = df['Latitude'].astype(float)
df['Longitude'] = df['Longitude'].astype(float)

# Scale latitude and longitude data
scaler_geo = StandardScaler()
geo_scaled = scaler_geo.fit_transform(df[['Latitude', 'Longitude']])

# Compute euclidean distances between latitude-longitude points
distances = euclidean_distances(geo_scaled)

# Perform K-means clustering
k = 3  # Number of clusters
kmeans = KMeans(n_clusters=k)
cluster_labels = kmeans.fit_predict(np.hstack((cosine_sim, distances)))

# Assign cluster labels back to the dataframe
df['Cluster'] = cluster_labels

# Set threshold for edge creation
threshold = 0.5  # Adjust this threshold based on your preferences

# Create a graph network plot for each cluster
for cluster in range(k):
    cluster_df = df[df['Cluster'] == cluster]

    # Create a graph
    G = nx.Graph()

    # Add nodes to the graph
    for index, row in cluster_df.iterrows():
        G.add_node(index, latitude=row['Latitude'], longitude=row['Longitude'])

    # Add edges based on cosine similarity and geographical distance
    for i in range(len(cluster_df)):
        for j in range(i + 1, len(cluster_df)):
            # Calculate edge weight based on a combination of cosine similarity and geographical distance
            edge_weight = cosine_sim[i, j] + (1 / (1 + distances[i, j]))  # Adjust this based on your preferences

            # Add edge if edge weight is above the threshold
            if edge_weight > threshold:
                G.add_edge(i, j, weight=edge_weight)

    # Plot the graph
    plt.figure(figsize=(10, 8))
    pos = nx.spring_layout(G)  # Layout for node positions
    nx.draw(G, pos, node_size=50, with_labels=False, edge_color='gray', width=0.5, alpha=0.7)
    plt.title(f'Graph Network for Cluster {cluster}')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.grid(True)
    plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import networkx as nx

# Assuming df is defined somewhere else in your code

# Handle NaN values in feature matrix
features = df.iloc[:, 9:].values
nan_columns = df.columns[9:][np.isnan(features).any(axis=0)]

nan_columns_count = df.isnull().sum()

# Replace NaN values with mean of respective columns
features = np.nan_to_num(features, nan=np.nanmean(features, axis=0))

# Scale feature data
features_scaled = StandardScaler().fit_transform(features)

# Compute cosine similarities of the features
cosine_sim = cosine_similarity(features_scaled)

# Preprocess geolocation column to extract latitude and longitude
df['Latitude'] = df['Geolocation'].str.extract(r'\((-?\d+\.\d+)')
df['Longitude'] = df['Geolocation'].str.extract(r'(-?\d+\.\d+)\)')

# Convert extracted latitude and longitude to float
df['Latitude'] = df['Latitude'].astype(float)
df['Longitude'] = df['Longitude'].astype(float)

# Scale latitude and longitude data
scaler_geo = StandardScaler()
geo_scaled = scaler_geo.fit_transform(df[['Latitude', 'Longitude']])

# Compute euclidean distances between latitude-longitude points
distances = euclidean_distances(geo_scaled)

# Perform K-means clustering
k = 3  # Number of clusters
kmeans = KMeans(n_clusters=k)
cluster_labels = kmeans.fit_predict(np.hstack((cosine_sim, distances)))

# Assign cluster labels back to the dataframe
df['Cluster'] = cluster_labels

# Set threshold for edge creation
threshold = 0.5  # Adjust this threshold based on your preferences

# Create a graph network plot for each cluster
for cluster in range(k):
    cluster_df = df[df['Cluster'] == cluster]

    # Create a graph
    G = nx.Graph()

    # Add nodes to the graph
    for index, row in cluster_df.iterrows():
        G.add_node(index, latitude=row['Latitude'], longitude=row['Longitude'])

    # Add edges based on cosine similarity and geographical distance
    for i in range(len(cluster_df)):
        for j in range(i + 1, len(cluster_df)):
            # Calculate edge weight based on a combination of cosine similarity and geographical distance
            edge_weight = cosine_sim[i, j] + (1 / (1 + distances[i, j]))  # Adjust this based on your preferences

            # Add edge if edge weight is above the threshold
            if edge_weight > threshold:
                G.add_edge(i, j, weight=edge_weight)

    # Plot the graph with edges
    plt.figure(figsize=(10, 8))
    pos = nx.spring_layout(G)  # Layout for node positions
    nx.draw(G, pos, node_size=50, with_labels=False, edge_color='gray', width=0.5, alpha=0.7)

    # Draw edges with weights above the threshold
    for edge in G.edges(data=True):
        if edge[2]['weight'] > threshold:
            nx.draw_networkx_edges(G, pos, edgelist=[(edge[0], edge[1])], width=edge[2]['weight']*2, edge_color='red')

    plt.title(f'Graph Network for Cluster {cluster}')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.grid(True)
    plt.show()

# Create separate DataFrames for each cluster
df0 = df[df['Cluster_Labels'] == 0]
df1 = df[df['Cluster_Labels'] == 1]
df2 = df[df['Cluster_Labels'] == 2]
df3 = df[df['Cluster_Labels'] == 3]

# Now you have separate DataFrames for each cluster

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Assuming 'X' is your data

# Calculate WCSS for different values of k
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(df[col])
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method graph
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()



#Hierarchial Clustering
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.metrics import silhouette_score

silhouette_scores = []
for i in range(2, 11):
    clustering = AgglomerativeClustering(n_clusters=i, affinity='euclidean', linkage='ward')
    cluster_labels = clustering.fit_predict(X)
    silhouette_scores.append(silhouette_score(X, cluster_labels))

plt.plot(range(2, 11), silhouette_scores)
plt.title('Silhouette Score Method')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.show()

from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
# Number of clusters
n_clusters = 4

# K-means clustering
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
kmeans_cluster_labels = kmeans.fit_predict(df[col])

from sklearn.cluster import KMeans

# Assuming kmeans is the fitted KMeans object
# Assuming df is your DataFrame

# Get the cluster labels
cluster_labels = kmeans.labels_

# Add the cluster labels as a new column to the DataFrame
df['Cluster_Labels'] = cluster_labels

# Now df contains a new column 'Cluster_Labels' with the cluster labels

df

# Create separate DataFrames for each cluster
df0 = df[df['Cluster_Labels'] == 0]
df1 = df[df['Cluster_Labels'] == 1]
df2 = df[df['Cluster_Labels'] == 2]
df3 = df[df['Cluster_Labels'] == 3]

# Now you have separate DataFrames for each cluster

df0

df1

df2

df3

df0.to_csv('/content/drive/My Drive/df0.csv', index=False)
df1.to_csv('/content/drive/My Drive/df1.csv', index=False)
df2.to_csv('/content/drive/My Drive/df2.csv', index=False)
df3.to_csv('/content/drive/My Drive/df3.csv', index=False)

