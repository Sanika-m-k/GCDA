# -*- coding: utf-8 -*-
"""education_financial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZjOQx9Wp98rs2H01EjzJSeJp_Brcd-kM
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd

data=pd.read_csv('/content/drive/MyDrive/df0.csv')

data

from scipy.stats import zscore, t
from sklearn.ensemble import IsolationForest
from scipy.stats import zscore

# Load your dataset
# Assuming your dataframe is named 'df'
# If your dataset is in a CSV file, you can use: df = pd.read_csv('your_dataset.csv')

# Extract the relevant columns for outlier detection
# For example, assuming you want to detect outliers in the 'ACS_TOT_HU' column
data_for_detection = data[['ACS_TOT_HU']]

# Standardize the data using Z-score
data_for_detection['Z_Score'] = zscore(data_for_detection['ACS_TOT_HU'])

def grubbs_test(data):
    n = len(data)
    mean = np.mean(data)
    std_dev = np.std(data)
    threshold = (n - 1) / np.sqrt(n) * np.sqrt((t.ppf((1 - 0.05 / (2 * n)), n - 2)) ** 2 / (n - 2 + (t.ppf((1 - 0.05 / (2 * n)), n - 2)) ** 2))

    # Calculate the test statistic
    test_statistic = np.abs((data - mean) / std_dev)

    # Identify outliers
    outliers = test_statistic > threshold

    return outliers

outliers_grubbs = grubbs_test(data_for_detection['ACS_TOT_HU'])

isolation_forest = IsolationForest(contamination=0.05)  # Adjust the contamination parameter as needed
data_for_detection['IsolationForest'] = isolation_forest.fit_predict(data_for_detection[['ACS_TOT_HU']])

# Identify the outliers from the Isolation Forest
outliers_isolation_forest = data_for_detection[data_for_detection['IsolationForest'] == -1]

# Display or save the results
print("Outliers detected using Grubbs' test:")
print(data[outliers_grubbs])

print("\nOutliers detected using Isolation Forest:")
print(outliers_isolation_forest)

import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset
# Assuming your dataframe is named 'df'
# If your dataset is in a CSV file, you can use: df = pd.read_csv('your_dataset.csv')

# Extract the relevant columns for outlier detection
# For example, assuming you want to detect outliers in the 'ACS_TOT_HU' column
data_for_detection = data[['ACS_TOT_HU']]

# Standardize the data using Z-score
data_for_detection['Z_Score'] = zscore(data_for_detection['ACS_TOT_HU'])

def grubbs_test(data):
    n = len(data)
    mean = np.mean(data)
    std_dev = np.std(data)
    threshold = (n - 1) / np.sqrt(n) * np.sqrt((t.ppf((1 - 0.05 / (2 * n)), n - 2)) ** 2 / (n - 2 + (t.ppf((1 - 0.05 / (2 * n)), n - 2)) ** 2))

    # Calculate the test statistic
    test_statistic = np.abs((data - mean) / std_dev)

    # Identify outliers
    outliers = test_statistic > threshold

    return outliers

outliers_grubbs = grubbs_test(data_for_detection['ACS_TOT_HU'])

import pandas as pd
import numpy as np
from scipy.stats import zscore, t
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset
# Assuming your dataframe is named 'df'
# If your dataset is in a CSV file, you can use: df = pd.read_csv('your_dataset.csv')

# Extract the relevant columns for outlier detection
# For example, assuming you want to detect outliers in the 'ACS_TOT_HU' column
data_for_detection = data[['ACS_TOT_HU']]

# Standardize the data using Z-score
data_for_detection['Z_Score'] = zscore(data_for_detection['ACS_TOT_HU'])

# Apply Grubbs' test for outlier detection
def grubbs_test(data):
    n = len(data)
    mean = np.mean(data)
    std_dev = np.std(data)
    threshold = (n - 1) / np.sqrt(n) * np.sqrt((t.ppf((1 - 0.05 / (2 * n)), n - 2)) ** 2 / (n - 2 + (t.ppf((1 - 0.05 / (2 * n)), n - 2)) ** 2))

    # Calculate the test statistic
    test_statistic = np.abs((data - mean) / std_dev)

    # Identify outliers
    outliers = test_statistic > threshold

    return outliers

outliers_grubbs = grubbs_test(data_for_detection['ACS_TOT_HU'])

# Apply Isolation Forest for outlier detection
# Assuming you want to use the 'ACS_TOT_HU' column for isolation forest
isolation_forest = IsolationForest(contamination=0.05)  # Adjust the contamination parameter as needed
data_for_detection['IsolationForest'] = isolation_forest.fit_predict(data_for_detection[['ACS_TOT_HU']])

# Identify the outliers from the Isolation Forest
outliers_isolation_forest = data_for_detection[data_for_detection['IsolationForest'] == -1]

# Plotting the results
plt.figure(figsize=(12, 6))

# Grubbs' test results
plt.subplot(1, 2, 1)
sns.histplot(data_for_detection['ACS_TOT_HU'], kde=True, color='blue', label='Original Data')
sns.histplot(data_for_detection.loc[outliers_grubbs, 'ACS_TOT_HU'], color='red', label='Grubbs\' Test Outliers')
plt.title('Outlier Detection - Grubbs\' Test')
plt.xlabel('ACS_TOT_HU')
plt.legend()

plt.subplot(1, 2, 2)
sns.histplot(data_for_detection['ACS_TOT_HU'], kde=True, color='blue', label='Original Data')
sns.histplot(data_for_detection.loc[outliers_isolation_forest.index, 'ACS_TOT_HU'], color='red', label='Isolation Forest Outliers')
plt.title('Outlier Detection - Isolation Forest')
plt.xlabel('ACS_TOT_HU')
plt.legend()

plt.tight_layout()
plt.show()

data_for_detection = data[['ACS_AVG_HH_SIZE']]

# Standardize the data using Z-score
data_for_detection['Z_Score'] = zscore(data_for_detection['ACS_AVG_HH_SIZE'])

# Apply Grubbs' test for outlier detection
def grubbs_test(data):
    n = len(data)
    mean = np.mean(data)
    std_dev = np.std(data)
    threshold = (n - 1) / np.sqrt(n) * np.sqrt((t.ppf((1 - 0.05 / (2 * n)), n - 2)) ** 2 / (n - 2 + (t.ppf((1 - 0.05 / (2 * n)), n - 2)) ** 2))

    # Calculate the test statistic
    test_statistic = np.abs((data - mean) / std_dev)

    # Identify outliers
    outliers = test_statistic > threshold

    return outliers

df=pd.read_csv('/content/HealthOutcomesBYCENSUSTRACT.csv')

df

y = df[[ 'Obesity among adults aged >=18 years',
 'Current asthma among adults aged >=18 years',
 'Current smoking among adults aged >=18 years',
 'Mammography use among women aged 50-74 years',
 'Fair or poor self-rated health status among adults aged >=18 years',
 'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',
 'Cervical cancer screening among adult women aged 21-65 years',
 'Physical health not good for >=14 days among adults aged >=18 years',
 'Visits to dentist or dental clinic among adults aged >=18 years',
 'Hearing disability among adults aged >=18 years',
 'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',
 'Sleeping less than 7 hours among adults aged >=18 years',
 'No leisure-time physical activity among adults aged >=18 years',
 'Cognitive disability among adults ages >=18 years',
 'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',
 'Coronary heart disease among adults aged >=18 years',
 'Depression among adults aged >=18 years',
 'High blood pressure among adults aged >=18 years',
 'Arthritis among adults aged >=18 years',
 'All teeth lost among adults aged >=65 years',
 'Cholesterol screening among adults aged >=18 years',
 'Chronic kidney disease among adults aged >=18 years',
 'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',
 'Mobility disability among adults aged >=18 years',
 'Diagnosed diabetes among adults aged >=18 years',
 'Independent living disability among adults aged >=18 years',
 'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure',
 'Binge drinking among adults aged >=18 years',
 'Stroke among adults aged >=18 years',
 'Vision disability among adults aged >=18 years',
 'Self-care disability among adults aged >=18 years',
 'Cancer (excluding skin cancer) among adults aged >=18 years',
 'Visits to doctor for routine checkup within the past year among adults aged >=18 years',
 'Mental health not good for >=14 days among adults aged >=18 years',
 'Chronic obstructive pulmonary disease among adults aged >=18 years',
 'Current lack of health insurance among adults aged 18-64 years',
 'Any disability among adults aged >=18 years']]

selected_columns = [
    'ACS_PCT_AIAN', 'ACS_PCT_AIAN_COMB', 'ACS_PCT_AIAN_NONHISP',
    'ACS_PCT_BLACK', 'ACS_PCT_BLACK_COMB', 'ACS_PCT_BLACK_NONHISP',
    'ACS_PCT_API_LANG', 'ACS_PCT_ASIAN', 'ACS_PCT_ASIAN_COMB',
    'ACS_PCT_ASIAN_NONHISP', 'ACS_PCT_WHITE', 'ACS_PCT_BLACK_FEMALE',
    'ACS_PCT_BLACK_MALE', 'ACS_PCT_FOREIGN_BORN', 'ACS_PCT_LT_HS',
    'ACS_PCT_POSTHS_ED'
]

# Select the specified columns
X = data[selected_columns]

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import pandas as pd
import matplotlib.pyplot as plt

# Assuming X and y are defined
# Fill missing values in X with the mean
for col in X.columns:
    if X[col].isnull().any():
        X[col] = X[col].fillna(X[col].median())

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the RandomForestRegressor model
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Get feature importances
feature_importance = model.feature_importances_

# Create a DataFrame to associate feature names with their importances
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

# Sort the features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
# plt.title('Feature Importance')
plt.show()

data.describe()

data.columns

data.columns = data.columns.str.strip()

for col in data.columns:
    print(str(data[col].value_counts()))
    print("-----------------------------------------")

data.nunique()

plt.figure(figsize=(8, 6));
sns.heatmap(data.isnull(), cmap='viridis')
plt.show()

# Check if 'Unnamed: 0' column exists in the DataFrame
if 'Unnamed: 0' in data.columns:
    # Drop the 'Unnamed: 0' column
    data.drop(columns=['Unnamed: 0'], inplace=True)

# Fill missing values in each column with the mean
for col in data.columns:
    if data[col].isnull().any():
        data[col] = data[col].fillna(data[col].mean())

data.isna().sum()

cat_columns = []
num_columns =[]

for col in data.columns:
    if data[col].dtype == 'object':
        cat_columns.append(col)
    else:
        num_columns.append(col)
print(cat_columns)
print(num_columns)

# Merge the two DataFrames on the common column
merged_data = pd.merge(data, df, left_on='TRACTFIPS', right_on='LocationID', how='inner')

# Drop the redundant common column (LocationID)
merged_data.drop('LocationID', axis=1, inplace=True)


# Assuming your DataFrame is called 'df'
# You can replace 'df' with the actual name of your DataFrame

# Drop totally duplicated rows
merged_data.drop_duplicates(inplace=True)

# Print the DataFrame after removing duplicates
#print(df)

# Print the merged DataFrame
merged_data

correlation_matrix = merged_data.corr()

# Print correlation matrix
correlation_matrix

# Define a threshold for strong correlation
threshold = 0.5

# Find strongly correlated pairs
strongly_correlated_pairs = []

for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            strongly_correlated_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))

# Create separate dataframes for each pair of strongly correlated variables

strongly_correlated_columns = []
for pair in strongly_correlated_pairs:
    if pair[0] not in strongly_correlated_columns:
        strongly_correlated_columns.append(pair[0])
    if pair[1] not in strongly_correlated_columns:
        strongly_correlated_columns.append(pair[1])

# Create a new dataframe with strongly correlated columns
new_df = merged_data[strongly_correlated_columns]

new_df

output_columns = [
    'Obesity among adults aged >=18 years',
    'Current asthma among adults aged >=18 years',
    'Current smoking among adults aged >=18 years',
    'Mammography use among women aged 50-74 years',
    'Fair or poor self-rated health status among adults aged >=18 years',
    'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',
    'Cervical cancer screening among adult women aged 21-65 years',
    'Physical health not good for >=14 days among adults aged >=18 years',
    'Visits to dentist or dental clinic among adults aged >=18 years',
    'Hearing disability among adults aged >=18 years',
    'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',

    'No leisure-time physical activity among adults aged >=18 years',
    'Cognitive disability among adults ages >=18 years',
    'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',
    'Coronary heart disease among adults aged >=18 years',
    'Depression among adults aged >=18 years',
    'High blood pressure among adults aged >=18 years',
    'Arthritis among adults aged >=18 years',
    'All teeth lost among adults aged >=65 years',
    'Cholesterol screening among adults aged >=18 years',
    'Chronic kidney disease among adults aged >=18 years',
    'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',
    'Mobility disability among adults aged >=18 years',
    'Diagnosed diabetes among adults aged >=18 years',
    'Independent living disability among adults aged >=18 years',
    'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure',
    'Binge drinking among adults aged >=18 years',
    'Stroke among adults aged >=18 years',
    'Vision disability among adults aged >=18 years',
    'Self-care disability among adults aged >=18 years',
    'Cancer (excluding skin cancer) among adults aged >=18 years',
    'Visits to doctor for routine checkup within the past year among adults aged >=18 years',
    'Mental health not good for >=14 days among adults aged >=18 years',
    'Chronic obstructive pulmonary disease among adults aged >=18 years',
    'Current lack of health insurance among adults aged 18-64 years',
    'Any disability among adults aged >=18 years'
]
# Add 'TRACTFIPS' to the list of columns
output_columns_with_tractfips = ['TRACTFIPS'] + output_columns

input_columns = [col for col in new_df.columns if col not in output_columns]

print(input_columns)

# Define the financial columns and output columns
financial_columns = ['TRACTFIPS', 'COUNTYFIPS', 'STATEFIPS',
                     'ACS_AVG_HH_SIZE', 'ACS_MEDIAN_HH_INC',
                      'ACS_MEDIAN_INC_F',
                     'ACS_MEDIAN_INC_M', 'ACS_MEDIAN_NONVET_INC',
                     'ACS_PER_CAPITA_INC', 'ACS_PCT_HH_1PERS',
                     'ACS_PCT_HH_ABOVE65', 'ACS_PCT_HH_ALONE_ABOVE65',
                     'ACS_PCT_HH_FOOD_STMP', 'ACS_PCT_HH_INC_10000',
                     'ACS_PCT_HH_INC_100000', 'ACS_PCT_HH_INC_99999',
                     'ACS_PCT_HH_PC', 'ACS_PCT_HH_PUB_ASSIST',
                     'ACS_PCT_HH_TABLET', 'ACS_PCT_HU_NO_VEH',
                     'ACS_PCT_RENTER_HU_COST_30PCT', 'ACS_PCT_RENTER_HU_COST_50PCT',
                     'ACS_PCT_VET_COLLEGE', 'ACS_PCT_VET_HS',
                     'ACS_TOT_HH', 'ACS_TOT_HU', 'ACS_TOT_POP_ABOVE15',
                     'ACS_TOT_POP_ABOVE16', 'ACS_TOT_POP_ABOVE25',
                     'ACS_TOT_POP_ABOVE5', 'ACS_TOT_POP_US_ABOVE1',
                     'ACS_TOT_POP_WT', 'ACS_TOT_WORKER_NWFH']

# Filter the columns from the original dataframe
financial_df = new_df[financial_columns + output_columns]

# Display the financial dataframe
print(financial_df.head())

data

g = sns.countplot(x="ACS_AVG_HH_SIZE",data=data)
plt.xticks()
plt.show()

fig = plt.figure(figsize=(30,30))

counter = 0

for col in num_columns:
    sub = fig.add_subplot(14,14,counter+1)
    g = sns.kdeplot(x=col,data=data,fill=True)
    plt.xticks()
    counter = counter + 1

financial_df.columns.to_list()

X = merged_data[['TRACTFIPS',
 'COUNTYFIPS',
 'STATEFIPS',
 'ACS_AVG_HH_SIZE',
 'ACS_MEDIAN_HH_INC',

 'ACS_MEDIAN_INC_F',
 'ACS_MEDIAN_INC_M',
 'ACS_MEDIAN_NONVET_INC',
 'ACS_PER_CAPITA_INC',
 'ACS_PCT_HH_1PERS',
 'ACS_PCT_HH_ABOVE65',
 'ACS_PCT_HH_ALONE_ABOVE65',
 'ACS_PCT_HH_FOOD_STMP',
 'ACS_PCT_HH_INC_10000',
 'ACS_PCT_HH_INC_100000',
 'ACS_PCT_HH_INC_99999',
 'ACS_PCT_HH_PC',
 'ACS_PCT_HH_PUB_ASSIST',
 'ACS_PCT_HH_TABLET',
 'ACS_PCT_HU_NO_VEH',
 'ACS_PCT_RENTER_HU_COST_30PCT',
 'ACS_PCT_RENTER_HU_COST_50PCT',
 'ACS_PCT_VET_COLLEGE',
 'ACS_PCT_VET_HS',
 'ACS_TOT_HH',
 'ACS_TOT_HU',
 'ACS_TOT_POP_ABOVE15',
 'ACS_TOT_POP_ABOVE16',
 'ACS_TOT_POP_ABOVE25',
 'ACS_TOT_POP_ABOVE5',
 'ACS_TOT_POP_US_ABOVE1',
 'ACS_TOT_POP_WT',
 'ACS_TOT_WORKER_NWFH',]]
y = merged_data[['Obesity among adults aged >=18 years',
 'Current asthma among adults aged >=18 years',
 'Current smoking among adults aged >=18 years',
 'Mammography use among women aged 50-74 years',
 'Fair or poor self-rated health status among adults aged >=18 years',
 'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',
 'Cervical cancer screening among adult women aged 21-65 years',
 'Physical health not good for >=14 days among adults aged >=18 years',
 'Visits to dentist or dental clinic among adults aged >=18 years',
 'Hearing disability among adults aged >=18 years',
 'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',

 'No leisure-time physical activity among adults aged >=18 years',
 'Cognitive disability among adults ages >=18 years',
 'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',
 'Coronary heart disease among adults aged >=18 years',
 'Depression among adults aged >=18 years',
 'High blood pressure among adults aged >=18 years',
 'Arthritis among adults aged >=18 years',
 'All teeth lost among adults aged >=65 years',
 'Cholesterol screening among adults aged >=18 years',
 'Chronic kidney disease among adults aged >=18 years',
 'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',
 'Mobility disability among adults aged >=18 years',
 'Diagnosed diabetes among adults aged >=18 years',
 'Independent living disability among adults aged >=18 years',
 'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure',
 'Binge drinking among adults aged >=18 years',
 'Stroke among adults aged >=18 years',
 'Vision disability among adults aged >=18 years',
 'Self-care disability among adults aged >=18 years',
 'Cancer (excluding skin cancer) among adults aged >=18 years',
 'Visits to doctor for routine checkup within the past year among adults aged >=18 years',
 'Mental health not good for >=14 days among adults aged >=18 years',
 'Chronic obstructive pulmonary disease among adults aged >=18 years',
 'Current lack of health insurance among adults aged 18-64 years',
 'Any disability among adults aged >=18 years']]

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer

# models
from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor
from xgboost import XGBRegressor

for col in y.columns:
    if y[col].isnull().any():
        y[col] = y[col].fillna(y[col].mean())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

y.info()

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

feature_importance = model.feature_importances_

# Create a DataFrame to associate feature names with their importances
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

# Sort the features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
# plt.title('Feature Importance')
plt.show()

y.isna().sum()

for col in y.columns:
    if y[col].isnull().any():
        y[col] = y[col].fillna(y[col].mean())

educational_columns = [
    'ACS_PCT_HH_BROADBAND',


'ACS_PCT_HH_CELLULAR',
'ACS_PCT_HH_CELLULAR_ONLY',

'ACS_PCT_HH_INTERNET',

'ACS_PCT_HH_NO_COMP_DEV',
'ACS_PCT_HH_NO_INTERNET',


'ACS_PCT_HH_PC',


'ACS_PCT_HH_SMARTPHONE',
'ACS_PCT_HH_SMARTPHONE_ONLY',
'ACS_PCT_HH_TABLET',

'ACS_PCT_RENTER_HU_COST_50PCT',
'ACS_PCT_WALK_2WORK',
'ACS_PCT_WORK_NO_CAR'
]

# Filter the columns from the original dataframe
educational_df = new_df[ educational_columns+output_columns]



# Display the transportation dataframe
print(educational_df)

educational_df.columns.to_list()

X = merged_data[['TRACTFIPS',
 'COUNTYFIPS',
 'STATEFIPS',
 'ACS_PCT_HH_BROADBAND',


'ACS_PCT_HH_CELLULAR',
'ACS_PCT_HH_CELLULAR_ONLY',

'ACS_PCT_HH_INTERNET',

'ACS_PCT_HH_NO_COMP_DEV',
'ACS_PCT_HH_NO_INTERNET',


'ACS_PCT_HH_PC',


'ACS_PCT_HH_SMARTPHONE',
'ACS_PCT_HH_SMARTPHONE_ONLY',
'ACS_PCT_HH_TABLET',

'ACS_PCT_RENTER_HU_COST_50PCT',
'ACS_PCT_WALK_2WORK',
'ACS_PCT_WORK_NO_CAR']]
y = merged_data[['Obesity among adults aged >=18 years',
 'Current asthma among adults aged >=18 years',
 'Current smoking among adults aged >=18 years',
 'Mammography use among women aged 50-74 years',
 'Fair or poor self-rated health status among adults aged >=18 years',
 'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',
 'Cervical cancer screening among adult women aged 21-65 years',
 'Physical health not good for >=14 days among adults aged >=18 years',
 'Visits to dentist or dental clinic among adults aged >=18 years',
 'Hearing disability among adults aged >=18 years',
 'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',

 'No leisure-time physical activity among adults aged >=18 years',
 'Cognitive disability among adults ages >=18 years',
 'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',
 'Coronary heart disease among adults aged >=18 years',
 'Depression among adults aged >=18 years',
 'High blood pressure among adults aged >=18 years',
 'Arthritis among adults aged >=18 years',
 'All teeth lost among adults aged >=65 years',
 'Cholesterol screening among adults aged >=18 years',
 'Chronic kidney disease among adults aged >=18 years',
 'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',
 'Mobility disability among adults aged >=18 years',
 'Diagnosed diabetes among adults aged >=18 years',
 'Independent living disability among adults aged >=18 years',
 'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure',
 'Binge drinking among adults aged >=18 years',
 'Stroke among adults aged >=18 years',
 'Vision disability among adults aged >=18 years',
 'Self-care disability among adults aged >=18 years',
 'Cancer (excluding skin cancer) among adults aged >=18 years',
 'Visits to doctor for routine checkup within the past year among adults aged >=18 years',
 'Mental health not good for >=14 days among adults aged >=18 years',
 'Chronic obstructive pulmonary disease among adults aged >=18 years',
 'Current lack of health insurance among adults aged 18-64 years',
 'Any disability among adults aged >=18 years']]

for col in y.columns:
    if y[col].isnull().any():
        y[col] = y[col].fillna(y[col].median())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

feature_importance = model.feature_importances_

# Create a DataFrame to associate feature names with their importances
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

# Sort the features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
# plt.title('Feature Importance')
plt.show()

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Assuming df is your DataFrame containing the features
# Drop the 'Unnamed: 0' and 'locationID' columns if they exist
columns_to_drop = ['Unnamed: 0', 'LocationID']
df.drop(columns=columns_to_drop, inplace=True, errors='ignore')

# Step 1: Handle missing values
# For example, impute missing values with mean
imputer = SimpleImputer(strategy='mean')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Step 2: Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_imputed)

# Apply PCA
pca = PCA()
pca.fit(scaled_data)

# Get the explained variance ratio and sort the indices
explained_variance_ratio = pca.explained_variance_ratio_
sorted_indices = explained_variance_ratio.argsort()[::-1]

# Get the most important feature
most_important_feature_index = sorted_indices[0]

# Print the most important feature
most_important_feature = df.columns[most_important_feature_index]
print("Most important feature:", most_important_feature)

df

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Assuming most_important_feature is your target variable (y)
y = merged_data[most_important_feature]

# Check for NaN values in y
if y.isnull().values.any():
    # Handle missing values in y, for example, impute with mean
    imputer = SimpleImputer(strategy='mean')
    y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))
    y = y_imputed.ravel()

# Convert y to categorical if it's continuous
y = pd.cut(y, bins=3, labels=['low', 'medium', 'high'])

# Assuming X contains your feature set
X = merged_data[['TRACTFIPS',
          'COUNTYFIPS',
          'STATEFIPS',
          'ACS_AVG_HH_SIZE',
          'ACS_MEDIAN_HH_INC',
          'ACS_MEDIAN_INC_F',
          'ACS_MEDIAN_INC_M',
          'ACS_MEDIAN_NONVET_INC',
          'ACS_PER_CAPITA_INC',
          'ACS_PCT_HH_1PERS',
          'ACS_PCT_HH_ABOVE65',
          'ACS_PCT_HH_ALONE_ABOVE65',
          'ACS_PCT_HH_FOOD_STMP',
          'ACS_PCT_HH_INC_10000',
          'ACS_PCT_HH_INC_100000',
          'ACS_PCT_HH_INC_99999',
          'ACS_PCT_HH_PC',
          'ACS_PCT_HH_PUB_ASSIST',
          'ACS_PCT_HH_TABLET',
          'ACS_PCT_HU_NO_VEH',
          'ACS_PCT_RENTER_HU_COST_30PCT',
          'ACS_PCT_RENTER_HU_COST_50PCT',
          'ACS_PCT_VET_COLLEGE',
          'ACS_PCT_VET_HS',
          'ACS_TOT_HH',
          'ACS_TOT_HU',
          'ACS_TOT_POP_ABOVE15',
          'ACS_TOT_POP_ABOVE16',
          'ACS_TOT_POP_ABOVE25',
          'ACS_TOT_POP_ABOVE5',
          'ACS_TOT_POP_US_ABOVE1',
          'ACS_TOT_POP_WT',
          'ACS_TOT_WORKER_NWFH']]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict the labels
y_pred = rf_classifier.predict(X_test)

# You can evaluate the model performance using classification metrics

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print(classification_report(y_test, y_pred))

# Feature Importance Plot
feature_importances = rf_classifier.feature_importances_
plt.figure(figsize=(10, 8))
plt.barh(X.columns, feature_importances)
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Feature Importance Plot')
plt.show()

# ROC Curve (for binary classification)
if len(set(y)) == 2:
    fpr, tpr, thresholds = roc_curve(y_test, y_pred)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()

pip install --upgrade scikit-learn

from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import plot_precision_recall_curve, plot_cumulative_gain
from sklearn.metrics import plot_roc_curve, plot_confusion_matrix
from sklearn.model_selection import learning_curve

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_pred)
average_precision = average_precision_score(y_test, y_pred)
plt.figure()
plt.step(recall, precision, where='post', color='b', alpha=0.5, label='Precision-Recall curve (AP = %0.2f)' % average_precision)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='upper right')
plt.show()

# Cumulative Gains Curve
plt.figure()
plot_cumulative_gain(rf_classifier, X_test, y_test)
plt.title('Cumulative Gains Curve')
plt.show()

# Learning Curve
train_sizes, train_scores, test_scores = learning_curve(rf_classifier, X, y, cv=5)
plt.figure()
plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', color='r', label='Training score')
plt.plot(train_sizes, test_scores.mean(axis=1), 'o-', color='g', label='Cross-validation score')
plt.xlabel('Training examples')
plt.ylabel('Score')
plt.title('Learning Curve')
plt.legend(loc='best')
plt.show()

# Additional Confusion Matrix
plt.figure()
plot_confusion_matrix(rf_classifier, X_test, y_test, cmap=plt.cm.Blues)
plt.title('Confusion Matrix (Additional)')
plt.show()

# Assume most_important_feature is the name of your most important feature
# Loop through each feature in the list and plot its relationship with the most important feature
for feature in ['TRACTFIPS', 'COUNTYFIPS', 'STATEFIPS', 'ACS_AVG_HH_SIZE', 'ACS_MEDIAN_HH_INC',
                'ACS_MEDIAN_INC_F', 'ACS_MEDIAN_INC_M', 'ACS_MEDIAN_NONVET_INC', 'ACS_PER_CAPITA_INC',
                'ACS_PCT_HH_1PERS', 'ACS_PCT_HH_ABOVE65', 'ACS_PCT_HH_ALONE_ABOVE65', 'ACS_PCT_HH_FOOD_STMP',
                'ACS_PCT_HH_INC_10000', 'ACS_PCT_HH_INC_100000', 'ACS_PCT_HH_INC_99999', 'ACS_PCT_HH_PC',
                'ACS_PCT_HH_PUB_ASSIST', 'ACS_PCT_HH_TABLET', 'ACS_PCT_HU_NO_VEH', 'ACS_PCT_RENTER_HU_COST_30PCT',
                'ACS_PCT_RENTER_HU_COST_50PCT', 'ACS_PCT_VET_COLLEGE', 'ACS_PCT_VET_HS', 'ACS_TOT_HH', 'ACS_TOT_HU',
                'ACS_TOT_POP_ABOVE15', 'ACS_TOT_POP_ABOVE16', 'ACS_TOT_POP_ABOVE25', 'ACS_TOT_POP_ABOVE5',
                'ACS_TOT_POP_US_ABOVE1', 'ACS_TOT_POP_WT', 'ACS_TOT_WORKER_NWFH']:

    plt.figure(figsize=(8, 6))
    plt.scatter(merged_data[most_important_feature], merged_data[feature], alpha=0.5)
    plt.xlabel('Most Important Feature')
    plt.ylabel(feature)
    plt.title(f'Relationship between Most Important Feature and {feature}')
    plt.show()

environmental_columns = [
    "ACS_PCT_HU_NO_VEH",
"ACS_PCT_PUBL_TRANSIT",


"ACS_PCT_PUB_COMMT_59MIN",
"ACS_PCT_PUB_COMMT_60MINUP",
"ACS_PCT_WALK_2WORK",
"ACS_PCT_WORK_NO_CAR",
"ACS_PCT_DISABLE"
]

# Filter the columns from the original dataframe
environmental_df = new_df[environmental_columns + output_columns]

# Display the financial dataframe
print(environmental_df.head())

X = merged_data[['TRACTFIPS',
 'COUNTYFIPS',
 'STATEFIPS',
 "ACS_PCT_HU_NO_VEH",
"ACS_PCT_PUBL_TRANSIT",


"ACS_PCT_PUB_COMMT_59MIN",
"ACS_PCT_PUB_COMMT_60MINUP",
"ACS_PCT_WALK_2WORK",
"ACS_PCT_WORK_NO_CAR",
"ACS_PCT_DISABLE"]]
y = merged_data[['Obesity among adults aged >=18 years',
 'Current asthma among adults aged >=18 years',
 'Current smoking among adults aged >=18 years',
 'Mammography use among women aged 50-74 years',
 'Fair or poor self-rated health status among adults aged >=18 years',
 'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',
 'Cervical cancer screening among adult women aged 21-65 years',
 'Physical health not good for >=14 days among adults aged >=18 years',
 'Visits to dentist or dental clinic among adults aged >=18 years',
 'Hearing disability among adults aged >=18 years',
 'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',

 'No leisure-time physical activity among adults aged >=18 years',
 'Cognitive disability among adults ages >=18 years',
 'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',
 'Coronary heart disease among adults aged >=18 years',
 'Depression among adults aged >=18 years',
 'High blood pressure among adults aged >=18 years',
 'Arthritis among adults aged >=18 years',
 'All teeth lost among adults aged >=65 years',
 'Cholesterol screening among adults aged >=18 years',
 'Chronic kidney disease among adults aged >=18 years',
 'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',
 'Mobility disability among adults aged >=18 years',
 'Diagnosed diabetes among adults aged >=18 years',
 'Independent living disability among adults aged >=18 years',
 'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure',
 'Binge drinking among adults aged >=18 years',
 'Stroke among adults aged >=18 years',
 'Vision disability among adults aged >=18 years',
 'Self-care disability among adults aged >=18 years',
 'Cancer (excluding skin cancer) among adults aged >=18 years',
 'Visits to doctor for routine checkup within the past year among adults aged >=18 years',
 'Mental health not good for >=14 days among adults aged >=18 years',
 'Chronic obstructive pulmonary disease among adults aged >=18 years',
 'Current lack of health insurance among adults aged 18-64 years',
 'Any disability among adults aged >=18 years']]

for col in y.columns:
    if y[col].isnull().any():
        y[col] = y[col].fillna(y[col].median())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

feature_importance = model.feature_importances_

# Create a DataFrame to associate feature names with their importances
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

# Sort the features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
# plt.title('Feature Importance')
plt.show()

legal_columns = [
    "ACS_PCT_FOREIGN_BORN",
"ACS_PCT_ENGL_NOT_ALL",
"ACS_PCT_ENGL_NOT_WELL",
"ACS_PCT_HEALTH_INC_ABOVE400",
"ACS_PCT_HEALTH_INC_BELOW137",
"ACS_PCT_LT_HS",
"ACS_PCT_POSTHS_ED",

"ACS_PCT_DISABLE",
"ACS_PCT_NONVET_DISABLE_18_64",

"ACS_MEDIAN_HH_INC",
"ACS_MEDIAN_NONVET_INC",
"ACS_PCT_MEDICAID_ANY",
"ACS_PCT_MEDICAID_ANY_BELOW64",
"ACS_PCT_PRIVATE_ANY",
"ACS_PCT_PRIVATE_ANY_BELOW64",
"ACS_PCT_PRIVATE_SELF",
"ACS_PCT_PRIVATE_SELF_BELOW64",
"ACS_PCT_PUBLIC_OTHER",
"ACS_PCT_SELF_MDCR_ABOVE35",
"ACS_PCT_CHILDREN_GRANDPARENT",
"ACS_PCT_HH_ABOVE65",
"ACS_PCT_HH_ALONE_ABOVE65"
]

# Filter the columns from the original dataframe
legal_df = new_df[legal_columns + output_columns]

# Display the financial dataframe
print(legal_df.head())

X = merged_data[['TRACTFIPS',
 'COUNTYFIPS',
 'STATEFIPS',
    "ACS_PCT_FOREIGN_BORN",
"ACS_PCT_ENGL_NOT_ALL",
"ACS_PCT_ENGL_NOT_WELL",
"ACS_PCT_HEALTH_INC_ABOVE400",
"ACS_PCT_HEALTH_INC_BELOW137",
"ACS_PCT_LT_HS",
"ACS_PCT_POSTHS_ED",

"ACS_PCT_DISABLE",
"ACS_PCT_NONVET_DISABLE_18_64",

"ACS_MEDIAN_HH_INC",
"ACS_MEDIAN_NONVET_INC",
"ACS_PCT_MEDICAID_ANY",
"ACS_PCT_MEDICAID_ANY_BELOW64",
"ACS_PCT_PRIVATE_ANY",
"ACS_PCT_PRIVATE_ANY_BELOW64",
"ACS_PCT_PRIVATE_SELF",
"ACS_PCT_PRIVATE_SELF_BELOW64",
"ACS_PCT_PUBLIC_OTHER",
"ACS_PCT_SELF_MDCR_ABOVE35",
"ACS_PCT_CHILDREN_GRANDPARENT",
"ACS_PCT_HH_ABOVE65",
"ACS_PCT_HH_ALONE_ABOVE65"]]
y = merged_data[['Obesity among adults aged >=18 years',
 'Current asthma among adults aged >=18 years',
 'Current smoking among adults aged >=18 years',
 'Mammography use among women aged 50-74 years',
 'Fair or poor self-rated health status among adults aged >=18 years',
 'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',
 'Cervical cancer screening among adult women aged 21-65 years',
 'Physical health not good for >=14 days among adults aged >=18 years',
 'Visits to dentist or dental clinic among adults aged >=18 years',
 'Hearing disability among adults aged >=18 years',
 'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',

 'No leisure-time physical activity among adults aged >=18 years',
 'Cognitive disability among adults ages >=18 years',
 'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',
 'Coronary heart disease among adults aged >=18 years',
 'Depression among adults aged >=18 years',
 'High blood pressure among adults aged >=18 years',
 'Arthritis among adults aged >=18 years',
 'All teeth lost among adults aged >=65 years',
 'Cholesterol screening among adults aged >=18 years',
 'Chronic kidney disease among adults aged >=18 years',
 'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',
 'Mobility disability among adults aged >=18 years',
 'Diagnosed diabetes among adults aged >=18 years',
 'Independent living disability among adults aged >=18 years',
 'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure',
 'Binge drinking among adults aged >=18 years',
 'Stroke among adults aged >=18 years',
 'Vision disability among adults aged >=18 years',
 'Self-care disability among adults aged >=18 years',
 'Cancer (excluding skin cancer) among adults aged >=18 years',
 'Visits to doctor for routine checkup within the past year among adults aged >=18 years',
 'Mental health not good for >=14 days among adults aged >=18 years',
 'Chronic obstructive pulmonary disease among adults aged >=18 years',
 'Current lack of health insurance among adults aged 18-64 years',
 'Any disability among adults aged >=18 years']]

for col in y.columns:
    if y[col].isnull().any():
        y[col] = y[col].fillna(y[col].median())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

feature_importance = model.feature_importances_

# Create a DataFrame to associate feature names with their importances
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

# Sort the features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
# plt.title('Feature Importance')
plt.show()

legal_columns = [
    "ACS_PCT_HH_BROADBAND",
"ACS_PCT_HH_BROADBAND_ANY",

"ACS_PCT_HH_CELLULAR",
"ACS_PCT_HH_CELLULAR_ONLY",

"ACS_PCT_HH_INTERNET",

"ACS_PCT_HH_NO_COMP_DEV",
"ACS_PCT_HH_NO_INTERNET",


"ACS_PCT_HH_PC",


"ACS_PCT_HH_SMARTPHONE",
"ACS_PCT_HH_SMARTPHONE_ONLY",
"ACS_PCT_HH_TABLET",

"ACS_PCT_MEDICAID_ANY",
"ACS_PCT_PRIVATE_ANY",
"ACS_PCT_PRIVATE_EMPL",
"ACS_PCT_CHILDREN_GRANDPARENT",

"ACS_PCT_HH_1PERS",
"ACS_PCT_HH_ABOVE65",
"ACS_PCT_HH_ALONE_ABOVE65",
"ACS_TOT_GRANDCHILDREN_GP",
"ACS_PCT_DISABLE",

"ACS_PCT_NONVET_DISABLE_18_64",

]

# Filter the columns from the original dataframe
legal_df = new_df[legal_columns + output_columns]

# Display the financial dataframe
print(legal_df.head())

X = merged_data[['TRACTFIPS',
 'COUNTYFIPS',
 'STATEFIPS',
    "ACS_PCT_HH_BROADBAND",
"ACS_PCT_HH_BROADBAND_ANY",

"ACS_PCT_HH_CELLULAR",
"ACS_PCT_HH_CELLULAR_ONLY",

"ACS_PCT_HH_INTERNET",

"ACS_PCT_HH_NO_COMP_DEV",
"ACS_PCT_HH_NO_INTERNET",


"ACS_PCT_HH_PC",


"ACS_PCT_HH_SMARTPHONE",
"ACS_PCT_HH_SMARTPHONE_ONLY",
"ACS_PCT_HH_TABLET",

"ACS_PCT_MEDICAID_ANY",
"ACS_PCT_PRIVATE_ANY",
"ACS_PCT_PRIVATE_EMPL",
"ACS_PCT_CHILDREN_GRANDPARENT",

"ACS_PCT_HH_1PERS",
"ACS_PCT_HH_ABOVE65",
"ACS_PCT_HH_ALONE_ABOVE65",
"ACS_TOT_GRANDCHILDREN_GP",
"ACS_PCT_DISABLE",

"ACS_PCT_NONVET_DISABLE_18_64",
]]
y = merged_data[['Obesity among adults aged >=18 years',
 'Current asthma among adults aged >=18 years',
 'Current smoking among adults aged >=18 years',
 'Mammography use among women aged 50-74 years',
 'Fair or poor self-rated health status among adults aged >=18 years',
 'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',
 'Cervical cancer screening among adult women aged 21-65 years',
 'Physical health not good for >=14 days among adults aged >=18 years',
 'Visits to dentist or dental clinic among adults aged >=18 years',
 'Hearing disability among adults aged >=18 years',
 'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',

 'No leisure-time physical activity among adults aged >=18 years',
 'Cognitive disability among adults ages >=18 years',
 'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',
 'Coronary heart disease among adults aged >=18 years',
 'Depression among adults aged >=18 years',
 'High blood pressure among adults aged >=18 years',
 'Arthritis among adults aged >=18 years',
 'All teeth lost among adults aged >=65 years',
 'Cholesterol screening among adults aged >=18 years',
 'Chronic kidney disease among adults aged >=18 years',
 'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',
 'Mobility disability among adults aged >=18 years',
 'Diagnosed diabetes among adults aged >=18 years',
 'Independent living disability among adults aged >=18 years',
 'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure',
 'Binge drinking among adults aged >=18 years',
 'Stroke among adults aged >=18 years',
 'Vision disability among adults aged >=18 years',
 'Self-care disability among adults aged >=18 years',
 'Cancer (excluding skin cancer) among adults aged >=18 years',
 'Visits to doctor for routine checkup within the past year among adults aged >=18 years',
 'Mental health not good for >=14 days among adults aged >=18 years',
 'Chronic obstructive pulmonary disease among adults aged >=18 years',
 'Current lack of health insurance among adults aged 18-64 years',
 'Any disability among adults aged >=18 years']]

for col in y.columns:
    if y[col].isnull().any():
        y[col] = y[col].fillna(y[col].median())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

feature_importance = model.feature_importances_

# Create a DataFrame to associate feature names with their importances
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

# Sort the features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
# plt.title('Feature Importance')
plt.show()

from itertools import combinations

# Define a function to generate polynomial feature names with a maximum degree of 2
def generate_poly_feature_names(feature_names):
    poly_feature_names = []
    for degree in range(1, 3):  # Degree 1 and 2
        for features in combinations(feature_names, degree):
            poly_feature_names.append('x'.join(features))
    return poly_feature_names

# Select the number of most important features to consider
num_most_important_features = 2
  # Adjust this number as needed

# Extract the names of the most important features
most_important_features = feature_importance_df['Feature'].head(num_most_important_features)

# Extract the most important features from the dataset
X_most_important = X[most_important_features]

# Create PolynomialFeatures transformer for feature engineering with degree=2
poly = PolynomialFeatures(degree=2, include_bias=False)

# Fit and transform the most important features to generate polynomial features
X_most_important_poly = poly.fit_transform(X_most_important)

# Generate names for the polynomial features manually
poly_feature_names = generate_poly_feature_names(most_important_features)

# Print the number of polynomial features and feature names for debugging
print("Number of polynomial features:", X_most_important_poly.shape[1])
print("Number of feature names:", len(poly_feature_names))

# Ensure that the number of features matches the number of feature names
assert X_most_important_poly.shape[1] == len(poly_feature_names), "Number of features and feature names mismatch"

# Create new DataFrame with the polynomial features
X_most_important_poly_df = pd.DataFrame(X_most_important_poly, columns=poly_feature_names)

# Concatenate the original DataFrame with the new polynomial features
X_combined = pd.concat([X, X_most_important_poly_df], axis=1)

# Drop the most important features used for engineering
X_combined.drop(most_important_features, axis=1, inplace=True)

# Now X_combined contains the original features along with the engineered polynomial features using the most important ones

# Assuming 'feature_importance_df' contains the DataFrame with feature importances

# Print the DataFrame to view feature names and their importances
print(feature_importance_df)

# Extract the names and importances of selected features
selected_feature_names = feature_importance_df['Feature']
selected_feature_importances = feature_importance_df['Importance']

# Print the names and importances of selected features
for feature_name, feature_importance in zip(selected_feature_names, selected_feature_importances):
    print(f"Feature: {feature_name}, Importance: {feature_importance}")

!pip install --upgrade scikit-learn

!pip uninstall scikit-learn
!pip install scikit-learn

from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_absolute_error

# Feature Selection
selected_features = SelectFromModel(model, prefit=True)
X_train_selected = selected_features.transform(X_train)
X_test_selected = selected_features.transform(X_test)

# Feature Engineering
poly = PolynomialFeatures(degree=2)
scaler = StandardScaler()

X_train_poly = poly.fit_transform(X_train_selected)
X_test_poly = poly.transform(X_test_selected)

X_train_scaled = scaler.fit_transform(X_train_poly)
X_test_scaled = scaler.transform(X_test_poly)

# Model Training
model.fit(X_train_scaled, y_train)

# Model Evaluation
y_pred = model.predict(X_test_scaled)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)

# Plot Feature Importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
plt.show()

def calculate_risk_score(y_pred, thresholds):
    """
    Calculate risk score based on predicted values and thresholds.

    Args:
    - y_pred (array-like): Predicted values.
    - thresholds (list): Thresholds for categorizing risk levels.

    Returns:
    - risk_scores (array-like): Risk scores calculated based on the predicted values and thresholds.
    """
    risk_scores = []
    for pred in y_pred:
        if pred < thresholds[0]:
            risk_scores.append('Low')
        elif pred < thresholds[1]:
            risk_scores.append('Medium')
        else:
            risk_scores.append('High')
    return risk_scores

# Define thresholds for categorizing risk levels
thresholds = [5, 10]  # Example thresholds

# Calculate risk scores
risk_scores = calculate_risk_score(y_pred, thresholds)

# Print risk scores
print("Risk Scores:", risk_scores)

from sklearn.preprocessing import PolynomialFeatures

# Interaction Features
interaction_poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_train_interactions = interaction_poly.fit_transform(X_train_selected)
X_test_interactions = interaction_poly.transform(X_test_selected)

# Combine interaction features with existing features
X_train_combined = np.concatenate((X_train_scaled, X_train_interactions), axis=1)
X_test_combined = np.concatenate((X_test_scaled, X_test_interactions), axis=1)

# Model Training with Interaction Features
model.fit(X_train_combined, y_train)

# Model Evaluation with Interaction Features
y_pred_interaction = model.predict(X_test_combined)
mae_interaction = mean_absolute_error(y_test, y_pred_interaction)
print("Mean Absolute Error with Interaction Features:", mae_interaction)

from sklearn.feature_selection import SelectFromModel

# Initialize SelectFromModel with RandomForestRegressor and specify threshold as 'mean'
selector = SelectFromModel(model, threshold='mean')

# Fit the selector to the data
selector.fit(X_train, y_train)

# Transform the data to select the most important features
X_train_selected = selector.transform(X_train)
X_test_selected = selector.transform(X_test)

# Print the shape of the transformed data to verify the number of selected features
print("Shape of X_train_selected:", X_train_selected.shape)
print("Shape of X_test_selected:", X_test_selected.shape)

# Get the indices of the selected features
selected_feature_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_feature_names = X.columns[selected_feature_indices]

# Print the names of the selected features
print("Selected Features:", selected_feature_names)

# Get the indices of the selected features
selected_feature_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_feature_names = X.columns[selected_feature_indices]

# Select the selected features from the original data
X_selected = X.iloc[:, selected_feature_indices]

# Combine selected features with the remaining ones
X_combined = pd.concat([X_selected, X.drop(X.columns[selected_feature_indices], axis=1)], axis=1)

# Now X_combined contains both the selected features and the remaining ones

X_combined

# Select the columns corresponding to the selected feature names excluding 'TRACTFIPS' and 'COUNTYFIPS'
selected_features = [feat for feat in selected_feature_names if feat not in ['TRACTFIPS', 'COUNTYFIPS']]

# Select the columns from X_combined based on the selected features
X_selected_two_features = X_combined[selected_features]

# Ensure that the resulting DataFrame contains only 2 features
if X_selected_two_features.shape[1] > 2:
    # If more than 2 features are selected, drop additional features
    X_selected_two_features = X_selected_two_features.iloc[:, :2]

# Now X_selected_two_features contains only 2 features based on the selected feature names, excluding 'TRACTFIPS' and 'COUNTYFIPS'

X_selected_two_features

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt

# Feature Engineering
poly = PolynomialFeatures(degree=2)
scaler = StandardScaler()

X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

X_train_scaled = scaler.fit_transform(X_train_poly)
X_test_scaled = scaler.transform(X_test_poly)

# Initialize a linear regression model
linear_model = LinearRegression()

# Initialize RFE with linear regression model
rfe_selector = RFE(estimator=linear_model, n_features_to_select=10, step=1)
rfe_selector = rfe_selector.fit(X_train_scaled, y_train)

# Get selected features
selected_features_rfe = rfe_selector.transform(X_train_scaled)
selected_features_test_rfe = rfe_selector.transform(X_test_scaled)

# Train the linear model using selected features
linear_model.fit(selected_features_rfe, y_train)

# Predict on test set
y_pred_rfe = linear_model.predict(selected_features_test_rfe)

# Evaluate model performance
mae_rfe = mean_absolute_error(y_test, y_pred_rfe)
print("Mean Absolute Error after RFE:", mae_rfe)

# Plot Feature Importances after RFE
plt.figure(figsize=(10, 6))
plt.barh(range(len(rfe_selector.support_)), rfe_selector.support_)
plt.xlabel('Feature Selected')
plt.ylabel('Feature Index')
plt.title('Recursive Feature Elimination (RFE)')
plt.show()



from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_absolute_error

# Feature Selection
selected_features = SelectFromModel(model, prefit=True)
X_train_selected = selected_features.transform(X_train)
X_test_selected = selected_features.transform(X_test)

# Feature Engineering
poly = PolynomialFeatures(degree=2)

X_train_poly = poly.fit_transform(X_train_selected)
X_test_poly = poly.transform(X_test_selected)

# Model Training
model.fit(X_train_poly, y_train)

# Predict using the trained model
y_pred = model.predict(X_test_poly)

# Model Evaluation
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)

# Plot Feature Importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
plt.show()

# Calculate the weighted average of each risk variable
weighted_avg_risk = np.dot(y_pred, feature_importances)

# Print the weighted average of each risk variable
print("Weighted Average of Each Risk Variable with Respect to Feature Importance:", weighted_avg_risk)

