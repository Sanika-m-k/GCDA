# -*- coding: utf-8 -*-
"""cluster0_discr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13gbywXExUD-bw2kXjTMQCrhrLQTHjU5P
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd

data=pd.read_csv('/content/drive/MyDrive/df0.csv')

data

df=pd.read_csv('/content/HealthOutcomesBYCENSUSTRACT.csv')

df

# Merge the two DataFrames on the common column
merged_data = pd.merge(data, df, left_on='TRACTFIPS', right_on='LocationID', how='inner')

# Drop the redundant common column (LocationID)
merged_data.drop('LocationID', axis=1, inplace=True)


# Assuming your DataFrame is called 'df'
# You can replace 'df' with the actual name of your DataFrame

# Drop totally duplicated rows
merged_data.drop_duplicates(inplace=True)

# Print the DataFrame after removing duplicates
print(df)

# Print the merged DataFrame
print(merged_data)

# Find numeric columns
numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns

# Replace NaN values with median for numeric columns
data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())

# Print the updated dataframe
print(data)

# List of variables
discrimination_variables = [
    "ACS_PCT_AIAN",
    "ACS_PCT_AIAN_COMB",
    "ACS_PCT_AIAN_NONHISP",
    "ACS_PCT_BLACK",
    "ACS_PCT_BLACK_COMB",
    "ACS_PCT_BLACK_NONHISP",
    "ACS_PCT_API_LANG",
    "ACS_PCT_ASIAN",
    "ACS_PCT_ASIAN_COMB",
    "ACS_PCT_ASIAN_NONHISP",
    "ACS_PCT_WHITE",
    "ACS_PCT_BLACK_FEMALE",
    "ACS_PCT_BLACK_MALE",
    "ACS_PCT_FOREIGN_BORN",
    "ACS_PCT_LT_HS",
    "ACS_PCT_POSTHS_ED"
]

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named data with your data
# Assuming variables and numeric_columns are defined correctly

# Check if the columns exist in your DataFrame
print("Variables:", discrimination_variables)

# Check the definition of numeric_columns
print("Numeric Columns:", numeric_columns)

# Concatenate variables and numeric columns into a single list
all_columns = discrimination_variables + numeric_columns.tolist()

# Selecting only the relevant columns
subset_df = data[all_columns]

# Calculating the correlation matrix
corr_matrix = subset_df.corr()

# Plotting the heatmap
plt.figure(figsize=(20, 20))

# Heatmap for correlation between all variables and numeric columns
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)

plt.title('Correlation between Variables and Numeric Columns')

plt.xticks(rotation=45)
plt.yticks(rotation=0)

plt.tight_layout()
plt.show()

numeric_columns

import pandas as pd

# Assuming you have a DataFrame named data with your data
# Assuming variables are defined correctly

# Selecting only the relevant columns
subset_df = data[discrimination_variables + numeric_columns]

# Calculating the correlation matrix
corr_matrix = subset_df.corr()

# Finding highly correlated variables
threshold = 0.7  # Set your correlation threshold
highly_correlated_variables = set()

for var in discrimination_variables:
    correlated_vars = corr_matrix[var][corr_matrix[var] > threshold].index.tolist()
    highly_correlated_variables.update(correlated_vars)

# Remove the variables that were originally in the list
highly_correlated_variables -= set(discrimination_variables)

print("Highly correlated variables:")
print(highly_correlated_variables)
#highly_correlated_legal = highly_correlated_variables

import pandas as pd

# Assuming you have a DataFrame named data with your data
# Assuming variables are defined correctly

# Selecting only the relevant columns
subset_df = data[discrimination_variables + list(numeric_columns)]
# Calculating the correlation matrix
corr_matrix = subset_df.corr()

# Finding highly correlated variables
threshold = 0.7  # Set your correlation threshold
highly_correlated_variables = set()

for var in discrimination_variables:
    correlated_vars = corr_matrix[var][corr_matrix[var] > threshold].index.tolist()
    highly_correlated_variables.update(correlated_vars)

# Remove the variables that were originally in the list
highly_correlated_variables -= set(discrimination_variables)

print("Highly correlated variables:")
print(highly_correlated_variables)
#highly_correlated_legal = highly_correlated_variables

print(len(highly_correlated_variables))

# Find numeric columns
numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns

# Replace NaN values with median for numeric columns
data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())

# Print the updated dataframe
print(data)

import matplotlib.pyplot as plt
import seaborn as sns

# Filter numeric columns from tn data
numeric_columns = data.select_dtypes(include=['int64', 'float64'])

fig = plt.figure(figsize=(30, 30))
counter = 0

# Limit the number of subplots to the capacity of the grid
for col in numeric_columns.columns:
    if counter >= 20:
        break  # Break the loop if we reach the maximum number of subplots
    sub = fig.add_subplot(5, 4, counter + 1)
    g = sns.kdeplot(x=numeric_columns[col], fill=True, ax=sub)
    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
    counter += 1

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Filter numeric columns from tn data
numeric_columns = merged_data.select_dtypes(include=['int64', 'float64'])

fig = plt.figure(figsize=(30, 30))
counter = 0

# Limit the number of subplots to the capacity of the grid
for col in numeric_columns.columns:
    if counter >= 20:
        break  # Break the loop if we reach the maximum number of subplots
    sub = fig.add_subplot(5, 4, counter + 1)
    g = sns.kdeplot(x=numeric_columns[col], fill=True, ax=sub)
    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
    counter += 1

plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
corr_matrix_merged = merged_data.corr()

# Plot the heatmap
plt.figure(figsize=(45,45))
sns.heatmap(corr_matrix_merged, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Merged Data')
plt.xlabel('Columns')
plt.ylabel('Columns')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

selected_columns = [
    'Obesity among adults aged >=18 years',
    'Current asthma among adults aged >=18 years',
    'Current smoking among adults aged >=18 years',
    'Mammography use among women aged 50-74 years',
 'Fair or poor self-rated health status among adults aged >=18 years',
 'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',
 'Cervical cancer screening among adult women aged 21-65 years',
 'Physical health not good for >=14 days among adults aged >=18 years',
 'Visits to dentist or dental clinic among adults aged >=18 years',
 'Hearing disability among adults aged >=18 years',
 'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',
 'Sleeping less than 7 hours among adults aged >=18 years',
 'No leisure-time physical activity among adults aged >=18 years',
 'Cognitive disability among adults ages >=18 years',
 'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',
 'Coronary heart disease among adults aged >=18 years',
 'Depression among adults aged >=18 years',
 'High blood pressure among adults aged >=18 years',
 'Arthritis among adults aged >=18 years',
 'All teeth lost among adults aged >=65 years',
 'Cholesterol screening among adults aged >=18 years',
 'Chronic kidney disease among adults aged >=18 years',
 'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',
 'Mobility disability among adults aged >=18 years',
 'Diagnosed diabetes among adults aged >=18 years',
 'Independent living disability among adults aged >=18 years',
 'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure',
 'Binge drinking among adults aged >=18 years',
 'Stroke among adults aged >=18 years',
 'Vision disability among adults aged >=18 years',
 'Self-care disability among adults aged >=18 years',
 'Cancer (excluding skin cancer) among adults aged >=18 years',
 'Visits to doctor for routine checkup within the past year among adults aged >=18 years',
 'Mental health not good for >=14 days among adults aged >=18 years',
 'Chronic obstructive pulmonary disease among adults aged >=18 years',
 'Current lack of health insurance among adults aged 18-64 years',
 'Any disability among adults aged >=18 years',
  "ACS_PCT_AIAN",
    "ACS_PCT_AIAN_COMB",
    "ACS_PCT_AIAN_NONHISP",
    "ACS_PCT_BLACK",
    "ACS_PCT_BLACK_COMB",
    "ACS_PCT_BLACK_NONHISP",
    "ACS_PCT_API_LANG",
    "ACS_PCT_ASIAN",
    "ACS_PCT_ASIAN_COMB",
    "ACS_PCT_ASIAN_NONHISP",
    "ACS_PCT_WHITE",
    "ACS_PCT_BLACK_FEMALE",
    "ACS_PCT_BLACK_MALE",
    "ACS_PCT_FOREIGN_BORN",
    "ACS_PCT_LT_HS",
    "ACS_PCT_POSTHS_ED"

]

data = merged_data.copy()

df1 = merged_data[selected_columns]

# Displaying the new DataFrame
df1

# Compute median values for each column
median_values = df1.median()

# Replace NaN values with median values
df1_filled = df1.fillna(median_values)

# Verify if NaN values have been replaced
print(df1_filled.isnull().sum())

import seaborn as sns
import matplotlib.pyplot as plt

# Plot heatmap
plt.figure(figsize=(40,40))
sns.heatmap(df1.corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of df1')
plt.show()

from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer  # Import SimpleImputer
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming merged_data is your DataFrame

# Drop non-numeric columns if needed
merged_data_numeric = merged_data.select_dtypes(include=['number'])

# Impute missing values with the median
imputer = SimpleImputer(strategy='median')  # Change the strategy to 'median'
merged_data_numeric_imputed = imputer.fit_transform(merged_data_numeric)

# Perform PCA
pca = PCA(n_components=2)  # Specify the number of components you want to reduce to
principal_components = pca.fit_transform(merged_data_numeric_imputed)

# Convert principal components to a DataFrame
principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])

# Plot the data in the reduced space
plt.figure(figsize=(10, 8))
sns.scatterplot(x='PC1', y='PC2', data=principal_df)
plt.title('Scatter Plot of Principal Components')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

# Set a threshold for correlation
threshold = 0.7

# Get the correlation matrix
correlation_matrix = df1.corr()

# Find highly correlated pairs with only one variable from legal_variables
highly_correlated_pairs = []

# Iterate over the upper triangle of the correlation matrix
for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        var1 = correlation_matrix.columns[i]
        var2 = correlation_matrix.columns[j]
        if (var1 in discrimination_variables) ^ (var2 in discrimination_variables) and abs(correlation_matrix.iloc[i, j]) > threshold:
            highly_correlated_pairs.append((var1, var2))

print("Highly correlated pairs with only one variable from discrimination_variables:")
for pair in highly_correlated_pairs:
    print(pair)

import seaborn as sns
import matplotlib.pyplot as plt

# Plot frequency graphs between highly correlated pairs
for pair in highly_correlated_pairs:
    plt.figure(figsize=(4,4))
    sns.histplot(data=df1, x=pair[0], color='skyblue', kde=True, label=pair[0])
    sns.histplot(data=df1, x=pair[1], color='orange', kde=True, label=pair[1])
    plt.title(f'Frequency Plot: {pair[0]} vs {pair[1]}')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.legend()
    plt.show()

import networkx as nx
import matplotlib.pyplot as plt

# Create an empty graph
G = nx.Graph()

# Add nodes and edges from highly correlated pairs
for pair in highly_correlated_pairs:
    G.add_edge(pair[0], pair[1])

# Plot the network graph
plt.figure(figsize=(10, 8))
nx.draw(G, with_labels=True, node_size=2000, node_color='skyblue', font_size=10, font_weight='bold')
plt.title('Highly Correlated Variables Network Graph')
plt.show()

merged_data

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Compute median values for each column
median_values = df1.median()

# Replace NaN values with median values
df1_filled = df1.fillna(median_values)

# Assuming df1_filled contains the data and legal_variables is a list of legal variables
# First, select only the legal variables from df1_filled
df1_discrimination = df1_filled[discrimination_variables]

# Select the remaining variables (excluding legal_variables)
df1_remaining = df1_filled.drop(columns=discrimination_variables)

# Get the number of samples
num_samples = len(df1_filled)

# Convert df1_legal and df1_remaining into feature vectors
X_legal = df1_discrimination.values.reshape(num_samples, -1)  # Convert legal variables to feature vectors
X_remaining = df1_remaining.values.reshape(num_samples, -1)  # Convert remaining variables to feature vectors

# If X_remaining has more columns than X_legal, truncate X_remaining
if X_remaining.shape[1] > X_legal.shape[1]:
    X_remaining = X_remaining[:, :X_legal.shape[1]]
# If X_legal has more columns than X_remaining, truncate X_legal
elif X_legal.shape[1] > X_remaining.shape[1]:
    X_legal = X_legal[:, :X_remaining.shape[1]]

# Calculate cosine similarity between legal_variables and df1_filled
cosine_similarity_value = cosine_similarity(X_legal, X_remaining)

# Print or use cosine_similarity_value as needed
print(cosine_similarity_value)

plt.figure(figsize=(8, 6))
sns.histplot(cosine_similarity_value.flatten(), bins=20, kde=True, color='skyblue')
plt.title('Distribution of Cosine Similarity Values')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Plot scatter plots between highly correlated pairs
for pair in highly_correlated_pairs:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=df1, x=pair[0], y=pair[1])
    plt.title(f'Scatter Plot: {pair[0]} vs {pair[1]}')
    plt.xlabel(pair[0])
    plt.ylabel(pair[1])
    plt.show()

from sklearn.impute import SimpleImputer

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean')
x_data_imputed = imputer.fit_transform(x_data)
y_data_imputed = imputer.fit_transform(y_data)

# Fit linear regression model
model = LinearRegression()
model.fit(x_data_imputed, y_data_imputed)

from sklearn.impute import SimpleImputer

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean')
x_data_imputed = imputer.fit_transform(x_data)
y_data_imputed = imputer.fit_transform(y_data)

# Fit linear regression model
model = LinearRegression()
model.fit(x_data_imputed, y_data_imputed)

# Get predictions
y_pred = model.predict(x_data_imputed)

# Plot the scatter plot and regression line
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df1, x=x_var, y=y_var)
plt.plot(x_data_imputed, y_pred, color='red', linewidth=2)  # Regression line
plt.title(f'Scatter Plot with Linear Regression: {x_var} vs {y_var}')
plt.xlabel(x_var)
plt.ylabel(y_var)
plt.show()

