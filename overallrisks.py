# -*- coding: utf-8 -*-
"""overallrisks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WUQPeEhPX772hyMSiq-BcKoR5vr7Efju
"""

import numpy as np
import pandas as pd

df0=pd.read_csv('/content/drive/MyDrive/Copy of cluster0.csv')

df0

df1=pd.read_csv('/content/drive/MyDrive/Copy of cluster1.csv')

df1

df2=pd.read_csv('/content/drive/MyDrive/Copy of cluster2.csv')

df2

import pandas as pd

# Assuming df0, df1, and df2 are your DataFrames

# Concatenate DataFrames along rows (vertically)
df = pd.concat([df0, df1, df2], axis=0)

# If the DataFrames have the same columns, you may want to reset the index
df.reset_index(drop=True, inplace=True)

# If you want to concatenate along columns (horizontally), use axis=1
# concatenated_df = pd.concat([df0, df1, df2], axis=1)

# Print the concatenated DataFrame
print(df)

df.drop(['Cluster_Labels'], inplace=True, axis=1)

df

for col in df.columns:
  print(col)

"""# combining df

"""

risk=['Mental_Health_RiskScore',
'Discrimination_RiskScore',
'Housing_RiskScore',
'SelfCare_RiskScore',
'FamilyConflicts_RiskScore',
'Transportation_RiskScore',
'Financial_RiskScore',
'Educational_RiskScore',

'Environmental_RiskScore',
'Technical_RiskScore'
      ]

df[risk]

from sklearn.preprocessing import MinMaxScaler

# Assuming df is your DataFrame and risk_list contains the columns you want to scale
scaler = MinMaxScaler()
df[risk] = scaler.fit_transform(df[risk])

df[risk]



import matplotlib.pyplot as plt

# Assuming risk_list contains the names of columns you want to plot against PCA_Variable
for column_name in risk:
    plt.figure(figsize=(8, 6))
    plt.scatter(df[column_name],df['PCA_Variable'], alpha=0.5)
    plt.title(f'PCA_Variable vs {column_name}')
    plt.ylabel('PCA_Variable')
    plt.xlabel(column_name)
    plt.grid(True)
    plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np

# Assuming df_plotting contains the DataFrame with the columns in the 'risk' list
for column_name in risk:
    X = df[[column_name]]

    # Define the number of clusters
    n_clusters = 3

    # Apply K-means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(X)

    # Get cluster labels
    cluster_labels = kmeans.labels_

    # Assign cluster labels to DataFrame
    df[f'Risk_Level_{column_name}'] = cluster_labels

    # Generate colors dynamically based on the number of unique cluster labels
    cluster_labels_unique = np.unique(cluster_labels)
    colors = plt.cm.viridis(np.linspace(0, 1, len(cluster_labels_unique)))

    # Plot the scatter plot with colored points
    plt.figure(figsize=(8, 6))
    for label, color in zip(cluster_labels_unique, colors):
        plt.scatter(X[column_name][cluster_labels == label], df['PCA_Variable'][cluster_labels == label], c=color, label=f'Cluster {label}')

    # Set labels and title
    plt.xlabel(column_name)
    plt.ylabel('PCA_Variable')
    plt.title(f'Clustering based on {column_name}')
    plt.colorbar(label='Cluster Label')
    plt.legend(title='Cluster', loc='upper right')
    plt.grid(True)
    plt.show()

df

level_mapping = {0: 'medium', 1: 'high', 2: 'low'}
df['Risk_Level_Mental_Health_RiskScore'] = df['Risk_Level_Mental_Health_RiskScore'].map(level_mapping)

level_mapping = {0: 'high', 1: 'low', 2: 'medium'}
df['Risk_Level_Discrimination_RiskScore'] = df['Risk_Level_Discrimination_RiskScore'].map(level_mapping)

level_mapping = {0: 'high', 1: 'low', 2: 'medium'}
df['Risk_Level_Housing_RiskScore'] = df['Risk_Level_Housing_RiskScore'].map(level_mapping)

level_mapping = {0: 'low', 1: 'high', 2: 'medium'}
df['Risk_Level_SelfCare_RiskScore'] = df['Risk_Level_SelfCare_RiskScore'].map(level_mapping)

level_mapping = {0: 'medium', 1: 'low', 2: 'high'}
df['Risk_Level_FamilyConflicts_RiskScore'] = df['Risk_Level_FamilyConflicts_RiskScore'].map(level_mapping)

level_mapping = {0: 'high', 1: 'low', 2: 'medium'}
df['Risk_Level_Transportation_RiskScore'] = df['Risk_Level_Transportation_RiskScore'].map(level_mapping)

level_mapping = {0: 'low', 1: 'high', 2: 'medium'}
df['Risk_Level_Financial_RiskScore'] = df['Risk_Level_Financial_RiskScore'].map(level_mapping)

level_mapping = {0: 'high', 1: 'low', 2: 'medium'}
df['Risk_Level_Educational_RiskScore'] = df['Risk_Level_Educational_RiskScore'].map(level_mapping)

level_mapping = {0: 'low', 1: 'high', 2: 'medium'}
df['Risk_Level_Legal_RiskScore'] = df['Risk_Level_Legal_RiskScore'].map(level_mapping)

level_mapping = {0: 'high', 1: 'low', 2: 'medium'}
df['Risk_Level_Environmental_RiskScore'] = df['Risk_Level_Environmental_RiskScore'].map(level_mapping)

level_mapping = {0: 'low', 1: 'high', 2: 'medium'}
df['Risk_Level_Technical_RiskScore'] = df['Risk_Level_Technical_RiskScore'].map(level_mapping)

df

geo=pd.read_csv('/content/FinalMasterDataset.csv')

geo

geo=geo[['TRACTFIPS','Latitude','Longitude']]
geo

# Assuming both DataFrames have a column named 'TRACTFIPS'
merged_df = pd.merge(df, geo, on='TRACTFIPS', how='left')
merged_df

null_values_count = merged_df['Latitude'].isnull().sum()

null_values_count

"""# Overall Risk"""

csv_filename = '/content/drive/My Drive/geo+risk.csv'  # Change this to your desired file path
merged_df.to_csv(csv_filename, index=False)

df=merged_df

df

out=pd.read_csv('/content/drive/MyDrive/Copy of HealthOutcomesBYCENSUSTRACT.csv')

out

merged_data = pd.merge(df, out , left_on='TRACTFIPS', right_on='LocationID', how='inner')

# Drop the redundant common column (LocationID)
merged_data.drop('LocationID', axis=1, inplace=True)


# Assuming your DataFrame is called 'df'
# You can replace 'df' with the actual name of your DataFrame

# Drop totally duplicated rows
merged_data.drop_duplicates(inplace=True)

# Print the DataFrame after removing duplicates
print(df)

# Print the merged DataFrame
print(merged_data)

data = merged_data.copy()

health_features=['Obesity among adults aged >=18 years',
 'Current asthma among adults aged >=18 years',
 'Current smoking among adults aged >=18 years',
 'Mammography use among women aged 50-74 years',
 'Fair or poor self-rated health status among adults aged >=18 years',
 'Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years',
 'Cervical cancer screening among adult women aged 21-65 years',
 'Physical health not good for >=14 days among adults aged >=18 years',
 'Visits to dentist or dental clinic among adults aged >=18 years',
 'Hearing disability among adults aged >=18 years',
 'Older adult women aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years',

 'No leisure-time physical activity among adults aged >=18 years',
 'Cognitive disability among adults ages >=18 years',
 'High cholesterol among adults aged >=18 years who have been screened in the past 5 years',
 'Coronary heart disease among adults aged >=18 years',
 'Depression among adults aged >=18 years',
 'High blood pressure among adults aged >=18 years',
 'Arthritis among adults aged >=18 years',
 'All teeth lost among adults aged >=65 years',
 'Cholesterol screening among adults aged >=18 years',
 'Chronic kidney disease among adults aged >=18 years',
 'Older adult men aged >=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening',
 'Mobility disability among adults aged >=18 years',
 'Diagnosed diabetes among adults aged >=18 years',
 'Independent living disability among adults aged >=18 years',
 'Taking medicine for high blood pressure control among adults aged >=18 years with high blood pressure',
 'Binge drinking among adults aged >=18 years',
 'Stroke among adults aged >=18 years',
 'Vision disability among adults aged >=18 years',
 'Self-care disability among adults aged >=18 years',
 'Cancer (excluding skin cancer) among adults aged >=18 years',
 'Visits to doctor for routine checkup within the past year among adults aged >=18 years',
 'Mental health not good for >=14 days among adults aged >=18 years',
 'Chronic obstructive pulmonary disease among adults aged >=18 years',
 'Current lack of health insurance among adults aged 18-64 years',
 'Any disability among adults aged >=18 years']

data

X=data[risk]
y=data[health_features]

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer

# models
from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor
from xgboost import XGBRegressor

for col in y.columns:
    if y[col].isnull().any():
        y[col] = y[col].fillna(y[col].mean())




X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

import matplotlib.pyplot as plt
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

feature_importance = model.feature_importances_

# Create a DataFrame to associate feature names with their importances
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

# Sort the features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
# plt.title('Feature Importance')
plt.show()

feature_importance_df

# Select the relevant columns from df_plotting
columns_of_interest = ['PCA_Variable'] + risk
subset_df = data[columns_of_interest]

# Calculate correlation matrix
correlation_matrix = subset_df.corr()

# Extract correlation of 'PCA_Variable' with all other features
pca_variable_corr = correlation_matrix['PCA_Variable']

# Display correlation values
print("Correlation between PCA_Variable and other features:")
print(pca_variable_corr)
pca_variable_corr1=pca_variable_corr[feature_importance_df['Feature']]
pca_variable_corr1

import seaborn as sns
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()



weighted_avgs = []
# Iterate over each row in df_plotting
for idx, row in data.iterrows():
    # Initialize a variable to store the sum of weighted values
    weighted_sum = 0

    # Iterate over each feature in the 'Feature' column of feature_importance_df
    for feature, weight in zip(feature_importance_df['Feature'], feature_importance_df['Importance']):
        # Calculate the weighted value for the current feature
        weighted_value = row[feature] * weight

        # Add the weighted value to the sum
        weighted_sum += weighted_value

    # Calculate the weighted average for the current row
    weighted_avg = weighted_sum / feature_importance_df['Importance'].sum()

    # Append the weighted average to the list
    weighted_avgs.append(weighted_avg)
# Add the list of weighted averages as a new column to df_plotting
df['Total_RiskScore'] = weighted_avgs

df

plt.figure(figsize=(8, 6))
plt.scatter(df['Total_RiskScore'], df['PCA_Variable'], alpha=0.5)
plt.title('Scatter Plot of Totak_RiskScore vs PCA_Variable')
plt.xlabel('Total_RiskScore')
plt.ylabel('PCA_Variable')
plt.grid(True)
plt.show()

data

